{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import sklearn\n",
    "import xgboost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\train.csv\")\n",
    "test=pd.read_csv(\"C:\\\\Users\\\\DELL\\\\Desktop\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the data.\n",
    "le=LabelEncoder()\n",
    "scaler=StandardScaler()\n",
    "train['Severity']=le.fit_transform(train['Severity'])\n",
    "train.iloc[:,0:10]=scaler.fit_transform(train.iloc[:,0:10])\n",
    "test.iloc[:,0:10]=scaler.fit_transform(test.iloc[:,0:10])\n",
    "#Dividing the data into train and test sets.\n",
    "x_train=train.iloc[:,0:10]\n",
    "y_train=train.iloc[:,-1]\n",
    "x_test=test.iloc[:,0:10]\n",
    "#initialising the return list of SVM parameters from PSO.\n",
    "pos=list()\n",
    "pos.append(0)\n",
    "pos.append(0)\n",
    "# function we are attempting to optimize (minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Particle Swarm Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmM(x0):\n",
    "    svc=svm.SVC(C=x0[0],gamma=x0[1])\n",
    "    x_tr,x_ts,y_tr, y_ts = train_test_split(x_train, y_train, test_size=0.2, random_state=0)\n",
    "    svc.fit(x_tr,y_tr)\n",
    "    y_pred=svc.predict(x_ts)\n",
    "    return accuracy_score(y_ts,y_pred)*-1\n",
    "def func1(x0):\n",
    "    return svmM(x0)    \n",
    "class Particle:\n",
    "    def __init__(self,x0):\n",
    "        self.position_i=[]          # particle position\n",
    "        self.velocity_i=[]          # particle velocity\n",
    "        self.pos_best_i=[]          # best position individual\n",
    "        self.err_best_i=-1          # best error individual\n",
    "        self.err_i=-1               # error individual\n",
    "\n",
    "        for i in range(0,num_dimensions):\n",
    "            self.velocity_i.append(random.uniform(-1,1))\n",
    "            self.position_i.append(x0[i])\n",
    "\n",
    "    # evaluate current fitness\n",
    "    def evaluate(self,costFunc):\n",
    "        self.err_i=costFunc(self.position_i)\n",
    "\n",
    "        # check to see if the current position is an individual best\n",
    "        if self.err_i < self.err_best_i or self.err_best_i==-1:\n",
    "            self.pos_best_i=self.position_i\n",
    "            self.err_best_i=self.err_i\n",
    "\n",
    "    # update new particle velocity\n",
    "    def update_velocity(self,pos_best_g):\n",
    "        w=0.5       # constant inertia weight (how much to weigh the previous velocity)\n",
    "        c1=1        # cognative constant\n",
    "        c2=2        # social constant\n",
    "\n",
    "        for i in range(0,num_dimensions):\n",
    "            r1=random.random()\n",
    "            r2=random.random()\n",
    "\n",
    "            vel_cognitive=c1*r1*(self.pos_best_i[i]-self.position_i[i])\n",
    "            vel_social=c2*r2*(pos_best_g[i]-self.position_i[i])\n",
    "            self.velocity_i[i]=w*self.velocity_i[i]+vel_cognitive+vel_social\n",
    "\n",
    "    # update the particle position based off new velocity updates\n",
    "    def update_position(self,bounds):\n",
    "        for i in range(0,num_dimensions):\n",
    "            self.position_i[i]=self.position_i[i]+self.velocity_i[i]\n",
    "\n",
    "            # adjust maximum position if necessary\n",
    "            if self.position_i[i]>bounds[i][1]:\n",
    "                self.position_i[i]=bounds[i][1]\n",
    "\n",
    "            # adjust minimum position if neseccary\n",
    "            if self.position_i[i] < bounds[i][0]:\n",
    "                self.position_i[i]=bounds[i][0]                \n",
    "class PSO():\n",
    "    def __init__(self,costFunc,x0,bounds,num_particles,maxiter):\n",
    "        global num_dimensions\n",
    "\n",
    "        num_dimensions=len(x0)\n",
    "        err_best_g=-1                   # best error for group\n",
    "        pos_best_g=[]                   # best position for group\n",
    "\n",
    "        # establish the swarm\n",
    "        swarm=[]\n",
    "        for i in range(0,num_particles):\n",
    "            swarm.append(Particle(x0))\n",
    "\n",
    "        # begin optimization loop\n",
    "        i=0\n",
    "        while i < maxiter:\n",
    "            #print i,err_best_g\n",
    "            # cycle through particles in swarm and evaluate fitness\n",
    "            for j in range(0,num_particles):\n",
    "                swarm[j].evaluate(costFunc)\n",
    "\n",
    "                # determine if current particle is the best (globally)\n",
    "                if swarm[j].err_i < err_best_g or err_best_g == -1:\n",
    "                    pos_best_g=list(swarm[j].position_i)\n",
    "                    err_best_g=float(swarm[j].err_i)\n",
    "\n",
    "            # cycle through swarm and update velocities and position\n",
    "            for j in range(0,num_particles):\n",
    "                swarm[j].update_velocity(pos_best_g)\n",
    "                swarm[j].update_position(bounds)\n",
    "            i+=1\n",
    "\n",
    "        # print final results\n",
    "        print ('FINAL:')\n",
    "        print (pos_best_g)\n",
    "        print (err_best_g)\n",
    "        pos[0]=pos_best_g[0]\n",
    "        pos[1]=pos_best_g[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Base Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLPerceptron(x_train,y_train,x_test):\n",
    "    mlp_clf=MLPClassifier(max_iter=2000)\n",
    "    mlp_clf.fit(x_train,y_train)\n",
    "    train_pred=mlp_clf.predict(x_train)\n",
    "    test_pred=mlp_clf.predict(x_test)\n",
    "    return test_pred,train_pred\n",
    "\n",
    "def RandomForest(x_train,y_train,x_test):\n",
    "    rf=RandomForestClassifier()\n",
    "    rf.fit(x_train,y_train)\n",
    "    train_pred=rf.predict(x_train)\n",
    "    test_pred=rf.predict(x_test)\n",
    "    return test_pred,train_pred\n",
    "\n",
    "#Applying SVM + Particle Swarm Optimisation\n",
    "def SVMachine(x_train,y_train,x_test):\n",
    "    initial=[1.0,0.1]               # initial starting location [x1,x2...]\n",
    "    bounds=[(1e-2,1e2),(1e-2,1e2)]  # input bounds [(x1_min,x1_max),(x2_min,x2_max)...]\n",
    "    PSO(func1,initial,bounds,num_particles=15,maxiter=30)\n",
    "    C=pos[0]\n",
    "    gamma=pos[1]\n",
    "    svc=svm.SVC(C=C,gamma=gamma)\n",
    "    svc.fit(x_train,y_train)\n",
    "    train_pred=svc.predict(x_train)\n",
    "    test_pred=svc.predict(x_test)\n",
    "    return test_pred,train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Base Classifier Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the 3 best base classification learners.\n",
    "test_pred1,train_pred1=MLPerceptron(x_train,y_train,x_test)\n",
    "test_pred2,train_pred2=SVMachine(x_train,y_train,x_test)\n",
    "test_pred3,train_pred3=RandomForest(x_train,y_train,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting results to dataframes.\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n",
    "train_pred3=pd.DataFrame(train_pred3)\n",
    "test_pred3=pd.DataFrame(test_pred3)\n",
    "#Applying the ensemble learner on the results of the base learners.(Also called META LEARNER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Logisitic Regression Ensemble Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n",
    "df_train=pd.concat([train_pred1,train_pred2,train_pred3],axis=1)\n",
    "df_test=pd.concat([test_pred1,test_pred2,test_pred3],axis=1)\n",
    "model.fit(df_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and store the Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final results prediction and writing. \n",
    "df=model.predict(df_test)\n",
    "df=le.inverse_transform(df)\n",
    "df=pd.DataFrame(df)\n",
    "path=\"C:\\\\Users\\\\DELL\\\\Desktop\\\\final_ans.csv\"\n",
    "df.to_csv(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
